{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IKCEST22 - 法俄泰与中文互译 Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前言\n",
    "\n",
    "见着大家报名积极，提交的人数却不多，于是整理了下部分代码，作为比赛基线，以供大家快速上分。 本项目使用使用transformer_base,在中 <-> 法俄泰 6个语向上分别训练了50轮。 祝飞桨的大伙在比赛上取得佳绩！是兄弟就来卷我吖！\n",
    "    \n",
    "![result](https://ai-studio-static-online.cdn.bcebos.com/0270ce6f69d742c2b772b516796f93f26b2ffa6096c54a3a8a5ece6ae683d6b7)\n",
    "    \n",
    "\n",
    "项目地址： [PaddleSeq](https://github.com/MiuGod0126/PaddleSeq/blob/main/examples/ikcest22/README.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 更新：\n",
    "\n",
    "**如有bug或不易使用的地方，欢迎大家提出宝贵意见！**\n",
    "\n",
    "9/27\n",
    "\n",
    "初赛快结束了，还能再卷下，使用四卡能得到极致的训练体验（两三小时），不需要修改命令，还没上榜的冲鸭鸭鸭。\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/29794ee0037a4ccd9adffb6441f34756549d4406dd9f4bcaaa83cdb1cf30f328)\n",
    "\n",
    "\n",
    "9/6: \n",
    "\n",
    "1.修正--arch加载错误bug，感谢[三岁](https://aistudio.baidu.com/aistudio/personalcenter/thirdview/284366)大佬的指正！\n",
    "\n",
    "2.应[三岁](https://aistudio.baidu.com/aistudio/personalcenter/thirdview/284366)和[笠雨聆月](https://aistudio.baidu.com/aistudio/personalcenter/thirdview/608082) 两位大佬的建议，简化了恢复训练的使用，从原先指定-\n",
    "resume、last-epoch、last-step变为直接加载权重目录下的yaml,即-c ckpt_dir/model.yaml，详见3.3恢复训练。\n",
    "\n",
    "3.应[笠雨聆月](https://aistudio.baidu.com/aistudio/personalcenter/thirdview/608082) 大佬的建议，添加了简易版本，丝滑的一键运行：\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/c877ae8f69a44a299898da7c709e93121bc9fc055a7746be906af29d3118fe5a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、赛事介绍\n",
    "\n",
    "[2022 IKCEST第四届“一带一路”国际大数据竞赛](https://aistudio.baidu.com/aistudio/competition/detail/477/0/introduction)\n",
    "\n",
    "本届大数据竞赛在中国工程院、教育部高等学校大学计算机课程教学指导委员会及丝绸之路大学联盟的指导下由联合国教科文组织国际工程科技知识中心（IKCEST）、中国工程科技知识中心（CKCEST）、百度公司及西安交通大学共同主办，旨在放眼“一带一路”倡议沿线国家，通过竞赛方式挖掘全球大数据人工智能尖端人才，实现政府—产业—高校合力推动大数据产业研究、应用、发展的目标，进一步夯实赛事的理论基础与实践基础，加快拔尖AI创新人才培养。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、数据处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 环境安装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: attrdict in /home/hang/anaconda3/envs/paddle/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (2.0.1)\n",
      "Requirement already satisfied: tqdm in /home/hang/anaconda3/envs/paddle/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (4.27.0)\n",
      "Requirement already satisfied: yacs in /home/hang/anaconda3/envs/paddle/lib/python3.8/site-packages (from -r requirements.txt (line 3)) (0.1.8)\n",
      "Requirement already satisfied: sacremoses==0.0.53 in /home/hang/anaconda3/envs/paddle/lib/python3.8/site-packages (from -r requirements.txt (line 4)) (0.0.53)\n",
      "Requirement already satisfied: fastcore==1.5.21 in /home/hang/anaconda3/envs/paddle/lib/python3.8/site-packages (from -r requirements.txt (line 5)) (1.5.21)\n",
      "Requirement already satisfied: sacrebleu==1.5 in /home/hang/anaconda3/envs/paddle/lib/python3.8/site-packages (from -r requirements.txt (line 6)) (1.5.0)\n",
      "Requirement already satisfied: pandas==1.1.5 in /home/hang/anaconda3/envs/paddle/lib/python3.8/site-packages (from -r requirements.txt (line 7)) (1.1.5)\n",
      "Requirement already satisfied: paddlenlp==2.1.1 in /home/hang/anaconda3/envs/paddle/lib/python3.8/site-packages (from -r requirements.txt (line 8)) (2.1.1)\n",
      "Requirement already satisfied: regex in /home/hang/anaconda3/envs/paddle/lib/python3.8/site-packages (from sacremoses==0.0.53->-r requirements.txt (line 4)) (2022.9.13)\n",
      "Requirement already satisfied: click in /home/hang/anaconda3/envs/paddle/lib/python3.8/site-packages (from sacremoses==0.0.53->-r requirements.txt (line 4)) (8.1.3)\n",
      "Requirement already satisfied: joblib in /home/hang/anaconda3/envs/paddle/lib/python3.8/site-packages (from sacremoses==0.0.53->-r requirements.txt (line 4)) (1.2.0)\n",
      "Requirement already satisfied: six in /home/hang/anaconda3/envs/paddle/lib/python3.8/site-packages (from sacremoses==0.0.53->-r requirements.txt (line 4)) (1.16.0)\n",
      "Requirement already satisfied: packaging in /home/hang/anaconda3/envs/paddle/lib/python3.8/site-packages (from fastcore==1.5.21->-r requirements.txt (line 5)) (21.3)\n",
      "Requirement already satisfied: pip in /home/hang/anaconda3/envs/paddle/lib/python3.8/site-packages (from fastcore==1.5.21->-r requirements.txt (line 5)) (22.1.2)\n",
      "Requirement already satisfied: portalocker in /home/hang/anaconda3/envs/paddle/lib/python3.8/site-packages (from sacrebleu==1.5->-r requirements.txt (line 6)) (2.5.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/hang/anaconda3/envs/paddle/lib/python3.8/site-packages (from pandas==1.1.5->-r requirements.txt (line 7)) (2022.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/hang/anaconda3/envs/paddle/lib/python3.8/site-packages (from pandas==1.1.5->-r requirements.txt (line 7)) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /home/hang/anaconda3/envs/paddle/lib/python3.8/site-packages (from pandas==1.1.5->-r requirements.txt (line 7)) (1.19.5)\n",
      "Requirement already satisfied: multiprocess in /home/hang/anaconda3/envs/paddle/lib/python3.8/site-packages (from paddlenlp==2.1.1->-r requirements.txt (line 8)) (0.70.13)\n",
      "Requirement already satisfied: colorlog in /home/hang/anaconda3/envs/paddle/lib/python3.8/site-packages (from paddlenlp==2.1.1->-r requirements.txt (line 8)) (6.7.0)\n",
      "Requirement already satisfied: paddlefsl==1.0.0 in /home/hang/anaconda3/envs/paddle/lib/python3.8/site-packages (from paddlenlp==2.1.1->-r requirements.txt (line 8)) (1.0.0)\n",
      "Requirement already satisfied: seqeval in /home/hang/anaconda3/envs/paddle/lib/python3.8/site-packages (from paddlenlp==2.1.1->-r requirements.txt (line 8)) (1.2.2)\n",
      "Requirement already satisfied: h5py in /home/hang/anaconda3/envs/paddle/lib/python3.8/site-packages (from paddlenlp==2.1.1->-r requirements.txt (line 8)) (3.7.0)\n",
      "Requirement already satisfied: colorama in /home/hang/anaconda3/envs/paddle/lib/python3.8/site-packages (from paddlenlp==2.1.1->-r requirements.txt (line 8)) (0.4.5)\n",
      "Requirement already satisfied: jieba in /home/hang/anaconda3/envs/paddle/lib/python3.8/site-packages (from paddlenlp==2.1.1->-r requirements.txt (line 8)) (0.42.1)\n",
      "Requirement already satisfied: requests~=2.24.0 in /home/hang/anaconda3/envs/paddle/lib/python3.8/site-packages (from paddlefsl==1.0.0->paddlenlp==2.1.1->-r requirements.txt (line 8)) (2.24.0)\n",
      "Requirement already satisfied: pillow==8.2.0 in /home/hang/anaconda3/envs/paddle/lib/python3.8/site-packages (from paddlefsl==1.0.0->paddlenlp==2.1.1->-r requirements.txt (line 8)) (8.2.0)\n",
      "Requirement already satisfied: PyYAML in /home/hang/anaconda3/envs/paddle/lib/python3.8/site-packages (from yacs->-r requirements.txt (line 3)) (6.0)\n",
      "Requirement already satisfied: dill>=0.3.5.1 in /home/hang/anaconda3/envs/paddle/lib/python3.8/site-packages (from multiprocess->paddlenlp==2.1.1->-r requirements.txt (line 8)) (0.3.5.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/hang/anaconda3/envs/paddle/lib/python3.8/site-packages (from packaging->fastcore==1.5.21->-r requirements.txt (line 5)) (3.0.9)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /home/hang/anaconda3/envs/paddle/lib/python3.8/site-packages (from seqeval->paddlenlp==2.1.1->-r requirements.txt (line 8)) (1.1.2)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/hang/anaconda3/envs/paddle/lib/python3.8/site-packages (from requests~=2.24.0->paddlefsl==1.0.0->paddlenlp==2.1.1->-r requirements.txt (line 8)) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/hang/anaconda3/envs/paddle/lib/python3.8/site-packages (from requests~=2.24.0->paddlefsl==1.0.0->paddlenlp==2.1.1->-r requirements.txt (line 8)) (2022.9.14)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/hang/anaconda3/envs/paddle/lib/python3.8/site-packages (from requests~=2.24.0->paddlefsl==1.0.0->paddlenlp==2.1.1->-r requirements.txt (line 8)) (1.25.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/hang/anaconda3/envs/paddle/lib/python3.8/site-packages (from requests~=2.24.0->paddlefsl==1.0.0->paddlenlp==2.1.1->-r requirements.txt (line 8)) (3.0.4)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /home/hang/anaconda3/envs/paddle/lib/python3.8/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp==2.1.1->-r requirements.txt (line 8)) (1.9.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/hang/anaconda3/envs/paddle/lib/python3.8/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp==2.1.1->-r requirements.txt (line 8)) (3.1.0)\n",
      "unzip:  cannot find or open ../nmt_data_tools.zip, ../nmt_data_tools.zip.zip or ../nmt_data_tools.zip.ZIP.\n",
      "Requirement already satisfied: lxml in /home/hang/anaconda3/envs/paddle/lib/python3.8/site-packages (from -r nmt_data_tools/requirements.txt (line 1)) (4.9.1)\n",
      "Requirement already satisfied: fasttext in /home/hang/anaconda3/envs/paddle/lib/python3.8/site-packages (from -r nmt_data_tools/requirements.txt (line 2)) (0.9.2)\n",
      "Requirement already satisfied: tqdm in /home/hang/anaconda3/envs/paddle/lib/python3.8/site-packages (from -r nmt_data_tools/requirements.txt (line 3)) (4.27.0)\n",
      "Requirement already satisfied: subword-nmt in /home/hang/anaconda3/envs/paddle/lib/python3.8/site-packages (from -r nmt_data_tools/requirements.txt (line 4)) (0.3.8)\n",
      "Requirement already satisfied: wget in /home/hang/anaconda3/envs/paddle/lib/python3.8/site-packages (from -r nmt_data_tools/requirements.txt (line 5)) (3.2)\n",
      "Requirement already satisfied: sacremoses in /home/hang/anaconda3/envs/paddle/lib/python3.8/site-packages (from -r nmt_data_tools/requirements.txt (line 6)) (0.0.53)\n",
      "Requirement already satisfied: zhconv in /home/hang/anaconda3/envs/paddle/lib/python3.8/site-packages (from -r nmt_data_tools/requirements.txt (line 7)) (1.4.3)\n",
      "Requirement already satisfied: pythainlp in /home/hang/anaconda3/envs/paddle/lib/python3.8/site-packages (from -r nmt_data_tools/requirements.txt (line 8)) (3.1.0)\n",
      "Requirement already satisfied: annoy in /home/hang/anaconda3/envs/paddle/lib/python3.8/site-packages (from -r nmt_data_tools/requirements.txt (line 9)) (1.17.1)\n",
      "Requirement already satisfied: simhash in /home/hang/anaconda3/envs/paddle/lib/python3.8/site-packages (from -r nmt_data_tools/requirements.txt (line 10)) (2.1.2)\n",
      "Requirement already satisfied: pybind11>=2.2 in /home/hang/anaconda3/envs/paddle/lib/python3.8/site-packages (from fasttext->-r nmt_data_tools/requirements.txt (line 2)) (2.10.0)\n",
      "Requirement already satisfied: numpy in /home/hang/anaconda3/envs/paddle/lib/python3.8/site-packages (from fasttext->-r nmt_data_tools/requirements.txt (line 2)) (1.19.5)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /home/hang/anaconda3/envs/paddle/lib/python3.8/site-packages (from fasttext->-r nmt_data_tools/requirements.txt (line 2)) (63.4.1)\n",
      "Requirement already satisfied: mock in /home/hang/anaconda3/envs/paddle/lib/python3.8/site-packages (from subword-nmt->-r nmt_data_tools/requirements.txt (line 4)) (4.0.3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: six in /home/hang/anaconda3/envs/paddle/lib/python3.8/site-packages (from sacremoses->-r nmt_data_tools/requirements.txt (line 6)) (1.16.0)\n",
      "Requirement already satisfied: click in /home/hang/anaconda3/envs/paddle/lib/python3.8/site-packages (from sacremoses->-r nmt_data_tools/requirements.txt (line 6)) (8.1.3)\n",
      "Requirement already satisfied: joblib in /home/hang/anaconda3/envs/paddle/lib/python3.8/site-packages (from sacremoses->-r nmt_data_tools/requirements.txt (line 6)) (1.2.0)\n",
      "Requirement already satisfied: regex in /home/hang/anaconda3/envs/paddle/lib/python3.8/site-packages (from sacremoses->-r nmt_data_tools/requirements.txt (line 6)) (2022.9.13)\n",
      "Requirement already satisfied: requests>=2.22.0 in /home/hang/anaconda3/envs/paddle/lib/python3.8/site-packages (from pythainlp->-r nmt_data_tools/requirements.txt (line 8)) (2.24.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/hang/anaconda3/envs/paddle/lib/python3.8/site-packages (from requests>=2.22.0->pythainlp->-r nmt_data_tools/requirements.txt (line 8)) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/hang/anaconda3/envs/paddle/lib/python3.8/site-packages (from requests>=2.22.0->pythainlp->-r nmt_data_tools/requirements.txt (line 8)) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/hang/anaconda3/envs/paddle/lib/python3.8/site-packages (from requests>=2.22.0->pythainlp->-r nmt_data_tools/requirements.txt (line 8)) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/hang/anaconda3/envs/paddle/lib/python3.8/site-packages (from requests>=2.22.0->pythainlp->-r nmt_data_tools/requirements.txt (line 8)) (2022.9.14)\n"
     ]
    }
   ],
   "source": [
    "#!unzip PaddleSeq.zip\n",
    "# !git clone  https://github.com/MiuGod0126/PaddleSeq.git\n",
    "\n",
    "!pip install -r requirements.txt\n",
    "!unzip ../nmt_data_tools.zip -d ./\n",
    "# !git clone https://github.com/MiuGod0126/nmt_data_tools.git\n",
    "!pip install -r nmt_data_tools/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 数据处理\n",
    "\n",
    "先对中泰分词（jieba/pythainlp），然后对所有语言应用subword-nmt分子词，并且从训练集中随机分出1000作为验证集，需要运行7分钟左右。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-09-30 13:39:29--  https://dataset-bj.cdn.bcebos.com/qianyan/datasets.tar.gz\n",
      "Resolving dataset-bj.cdn.bcebos.com (dataset-bj.cdn.bcebos.com)... 101.72.203.35, 112.65.203.35\n",
      "Connecting to dataset-bj.cdn.bcebos.com (dataset-bj.cdn.bcebos.com)|101.72.203.35|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 66338319 (63M) [application/x-gzip]\n",
      "Saving to: ‘datasets.tar.gz’\n",
      "\n",
      "datasets.tar.gz     100%[===================>]  63.26M  24.3MB/s    in 2.6s    \n",
      "\n",
      "2022-09-30 13:39:32 (24.3 MB/s) - ‘datasets.tar.gz’ saved [66338319/66338319]\n",
      "\n",
      "datasets/\n",
      "datasets/ru_zh.train\n",
      "datasets/zh_ru.train\n",
      "datasets/th_zh.train\n",
      "datasets/zh_fr.test\n",
      "datasets/zh_ru.test\n",
      "datasets/zh_th.test\n",
      "datasets/evaluation.zip\n",
      "datasets/reader.py\n",
      "datasets/th_zh.test\n",
      "datasets/zh_fr.train\n",
      "datasets/README\n",
      "datasets/fr_zh.train\n",
      "datasets/ru_zh.test\n",
      "datasets/zh_th.train\n",
      "datasets/fr_zh.test\n",
      "100%|██████████████████████████████| 100000/100000 [00:00<00:00, 1979294.89it/s]\n",
      "write to datasets/raw/zh_th/train.zh success, total 99000 lines.\n",
      "write to datasets/raw/zh_th/train.th success, total 99000 lines.\n",
      "write to datasets/raw/zh_th/dev.zh success, total 1000 lines.\n",
      "write to datasets/raw/zh_th/dev.th success, total 1000 lines.\n",
      "100%|██████████████████████████████| 100000/100000 [00:00<00:00, 1964646.42it/s]\n",
      "write to datasets/raw/zh_fr/train.zh success, total 99000 lines.\n",
      "write to datasets/raw/zh_fr/train.fr success, total 99000 lines.\n",
      "write to datasets/raw/zh_fr/dev.zh success, total 1000 lines.\n",
      "write to datasets/raw/zh_fr/dev.fr success, total 1000 lines.\n",
      "100%|██████████████████████████████| 100000/100000 [00:00<00:00, 2036949.94it/s]\n",
      "write to datasets/raw/zh_ru/train.zh success, total 99000 lines.\n",
      "write to datasets/raw/zh_ru/train.ru success, total 99000 lines.\n",
      "write to datasets/raw/zh_ru/dev.zh success, total 1000 lines.\n",
      "write to datasets/raw/zh_ru/dev.ru success, total 1000 lines.\n",
      "tokenize datasets/raw/zh_fr//train.zh ...\n",
      "0it [00:00, ?it/s]Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.823 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.876 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.933 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.981 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "99000it [00:04, 21291.78it/s]\n",
      "----learn bpe code, and apply to datasets/tmp/zh_fr/train.tok.zh----\n",
      "100%|####################################| 18000/18000 [00:25<00:00, 717.85it/s]\n",
      "datasets/tmp/zh_fr/train.bpe.zh\n",
      "Processing datasets/tmp/zh_fr/train.bpe.zh\n",
      "Done\n",
      "write to datasets/bpe/zh_fr/vocab.zh success, total 23438 lines.\n",
      "tokenize datasets/raw/zh_fr//valid.zh ...\n",
      "0it [00:00, ?it/s]Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.678 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "1000it [00:00, 1160.58it/s]\n",
      "----apply bpe to datasets/tmp/zh_fr/valid.tok.zh----\n",
      "tokenize datasets/raw/zh_fr//test.zh_fr.zh ...\n",
      "0it [00:00, ?it/s]Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.693 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "1000it [00:00, 1146.14it/s]\n",
      "----apply bpe to datasets/tmp/zh_fr/test.zh_fr.tok.zh----\n",
      "tokenize datasets/raw/zh_fr//train.fr ...\n",
      "language fr dont's need cut.\n",
      "----learn bpe code, and apply to datasets/tmp/zh_fr/train.tok.fr----\n",
      "100%|####################################| 18000/18000 [00:46<00:00, 390.73it/s]\n",
      "datasets/tmp/zh_fr/train.bpe.fr\n",
      "Processing datasets/tmp/zh_fr/train.bpe.fr\n",
      "Done\n",
      "write to datasets/bpe/zh_fr/vocab.fr success, total 18460 lines.\n",
      "tokenize datasets/raw/zh_fr//valid.fr ...\n",
      "language fr dont's need cut.\n",
      "----apply bpe to datasets/tmp/zh_fr/valid.tok.fr----\n",
      "tokenize datasets/raw/zh_fr//test.fr_zh.fr ...\n",
      "language fr dont's need cut.\n",
      "----apply bpe to datasets/tmp/zh_fr/test.fr_zh.tok.fr----\n",
      "tokenize datasets/raw/zh_ru//train.zh ...\n",
      "0it [00:00, ?it/s]Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.697 seconds.\n",
      "Loading model cost 0.699 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.760 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "1it [00:00,  1.08it/s]Loading model cost 1.047 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "99000it [00:05, 17666.47it/s]\n",
      "----learn bpe code, and apply to datasets/tmp/zh_ru/train.tok.zh----\n",
      "100%|####################################| 18000/18000 [00:23<00:00, 751.63it/s]\n",
      "datasets/tmp/zh_ru/train.bpe.zh\n",
      "Processing datasets/tmp/zh_ru/train.bpe.zh\n",
      "Done\n",
      "write to datasets/bpe/zh_ru/vocab.zh success, total 24887 lines.\n",
      "tokenize datasets/raw/zh_ru//valid.zh ...\n",
      "0it [00:00, ?it/s]Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.707 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "1000it [00:00, 1062.84it/s]\n",
      "----apply bpe to datasets/tmp/zh_ru/valid.tok.zh----\n",
      "tokenize datasets/raw/zh_ru//test.zh_ru.zh ...\n",
      "0it [00:00, ?it/s]Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.688 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "1000it [00:00, 1100.91it/s]\n",
      "----apply bpe to datasets/tmp/zh_ru/test.zh_ru.tok.zh----\n",
      "tokenize datasets/raw/zh_ru//train.ru ...\n",
      "language ru dont's need cut.\n",
      "----learn bpe code, and apply to datasets/tmp/zh_ru/train.tok.ru----\n",
      "100%|####################################| 18000/18000 [01:05<00:00, 273.23it/s]\n",
      "datasets/tmp/zh_ru/train.bpe.ru\n",
      "Processing datasets/tmp/zh_ru/train.bpe.ru\n",
      "Done\n",
      "write to datasets/bpe/zh_ru/vocab.ru success, total 18610 lines.\n",
      "tokenize datasets/raw/zh_ru//valid.ru ...\n",
      "language ru dont's need cut.\n",
      "----apply bpe to datasets/tmp/zh_ru/valid.tok.ru----\n",
      "tokenize datasets/raw/zh_ru//test.ru_zh.ru ...\n",
      "language ru dont's need cut.\n",
      "----apply bpe to datasets/tmp/zh_ru/test.ru_zh.tok.ru----\n",
      "tokenize datasets/raw/zh_th//train.zh ...\n",
      "0it [00:00, ?it/s]Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.701 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.700 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.701 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.705 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "99000it [00:03, 31826.97it/s]\n",
      "----learn bpe code, and apply to datasets/tmp/zh_th/train.tok.zh----\n",
      "100%|####################################| 18000/18000 [00:28<00:00, 637.26it/s]\n",
      "datasets/tmp/zh_th/train.bpe.zh\n",
      "Processing datasets/tmp/zh_th/train.bpe.zh\n",
      "Done\n",
      "write to datasets/bpe/zh_th/vocab.zh success, total 24048 lines.\n",
      "tokenize datasets/raw/zh_th//valid.zh ...\n",
      "0it [00:00, ?it/s]Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.684 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "1000it [00:00, 1233.35it/s]\n",
      "----apply bpe to datasets/tmp/zh_th/valid.tok.zh----\n",
      "tokenize datasets/raw/zh_th//test.zh_th.zh ...\n",
      "0it [00:00, ?it/s]Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.694 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "1000it [00:00, 1221.00it/s]\n",
      "----apply bpe to datasets/tmp/zh_th/test.zh_th.tok.zh----\n",
      "tokenize datasets/raw/zh_th//train.th ...\n",
      "99000it [00:03, 28128.96it/s]\n",
      "----learn bpe code, and apply to datasets/tmp/zh_th/train.tok.th----\n",
      "100%|####################################| 18000/18000 [00:18<00:00, 954.16it/s]\n",
      "datasets/tmp/zh_th/train.bpe.th\n",
      "Processing datasets/tmp/zh_th/train.bpe.th\n",
      "Done\n",
      "write to datasets/bpe/zh_th/vocab.th success, total 16194 lines.\n",
      "tokenize datasets/raw/zh_th//valid.th ...\n",
      "1000it [00:00, 4832.98it/s]\n",
      "----apply bpe to datasets/tmp/zh_th/valid.tok.th----\n",
      "tokenize datasets/raw/zh_th//test.th_zh.th ...\n",
      "1000it [00:00, 4687.09it/s]\n",
      "----apply bpe to datasets/tmp/zh_th/test.th_zh.tok.th----\n",
      "all done!\n"
     ]
    }
   ],
   "source": [
    "!bash examples/ikcest22/scripts/prepare-ikcest22.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bpe  evaluation.zip  raw  reader.py  README  tmp\n",
      "初临异界头像框\n",
      "初临 异界头 像框\n",
      "初@@ 临 异@@ 界@@ 头 像框\n"
     ]
    }
   ],
   "source": [
    "# 数据在datasets下，raw是原始未处理的文本对；tmp完成了分词和bpe；最后训练数据写入到bpe文件夹\n",
    "!ls datasets/\n",
    "!head -n 1 datasets/raw/zh_th/train.zh\n",
    "!head -n 1 datasets/tmp/zh_th/train.tok.zh\n",
    "!head -n 1 datasets/tmp/zh_th/train.bpe.zh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code.th  test.th_zh.th\ttrain.th  valid.th  vocab.th\r\n",
      "code.zh  test.zh_th.zh\ttrain.zh  valid.zh  vocab.zh\r\n"
     ]
    }
   ],
   "source": [
    "# 本项目的数据格式如下，语言对被分成prefix.lang两个文件，其中valid是随机分的1000句，code是bpe词表。\n",
    "!ls datasets/bpe/zh_th/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 数据加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'paddle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 加载中泰数据\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpaddleseq\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m prep_dataset, prep_loader\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01myacs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CfgNode\n\u001b[1;32m      5\u001b[0m cfg_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexamples/ikcest22/configs/zh_th.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/桌面/PaddleSeq/paddleseq/reader/__init__.py:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindexed_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m~/桌面/PaddleSeq/paddleseq/reader/indexed_dataset.py:4\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mshutil\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpaddle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfunctools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lru_cache\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfile_io\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PathManager\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'paddle'"
     ]
    }
   ],
   "source": [
    "# 加载中泰数据\n",
    "\n",
    "from paddleseq.reader import prep_dataset, prep_loader\n",
    "from yacs.config import CfgNode\n",
    "cfg_path=\"examples/ikcest22/configs/zh_th.yaml\"\n",
    "conf = CfgNode.load_cfg(open(cfg_path, encoding=\"utf-8\"))\n",
    "dataset_train = prep_dataset(conf, mode=\"train\")\n",
    "dataset_valid = prep_dataset(conf, mode=\"dev\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "配置文件\n",
    "\n",
    "```\n",
    "SAVE: output\n",
    "data:\n",
    "  has_target: True\n",
    "  lang_embed: False\n",
    "  lazy_load: False\n",
    "  pad_factor: 8\n",
    "  pad_vocab: False\n",
    "  special_token: ['<s>', '<pad>', '</s>', '<unk>']\n",
    "  src_bpe_path: datasets/bpe/zh_th/code.zh\n",
    "  src_lang: zh\n",
    "  test_pref: datasets/bpe/zh_th/test.zh_th\n",
    "  tgt_bpe_path: datasets/bpe/zh_th/code.th\n",
    "  tgt_lang: th\n",
    "  train_pref: datasets/bpe/zh_th/train\n",
    "  truecase_path: None\n",
    "  use_binary: False\n",
    "  use_moses_bpe: False\n",
    "  valid_pref: datasets/bpe/zh_th/valid\n",
    "  vocab_pref: datasets/bpe/zh_th/vocab\n",
    " ......\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 0, 'src': '变 身 期间 ， 对 所有 BOSS 伤害 增加 7%', 'tgt': 'ใน ช่วง กลายร่าง ดา เม จ ที่ ทำ ใส่ BOSS ทั้งหมด เพิ่มขึ้น 7 %'}\n",
      "{'id': 1, 'src': '我 开始 烦 了 ， 老鼠 们 ！ 尝尝 来自 坎@@ 特 鲁 最高 科学 的 力量 ！', 'tgt': 'ข้า เริ่ม รำ คาน แล้ว นะ พวก หนู ! มา ลอง พลัง แห่ง เทคโนโลยี สูงสุด ของ คัท@@ ลู@@ !'}\n",
      "{'id': 2, 'src': '而 后面 是 带 肩@@ 线 的 设计 ， 慵懒 中 又 带 几分 利落 ， 做 了 个 超长 款 ， 显得 人 线条 修长 。', 'tgt': 'การ ออกแบบ ด้านหลัง ที่ มี เส้น ไหล่ ภายใน ความ@@ ขี้เกียจ ติด ความ เรียบร้อย บ้าง ทำเป็น แบบ ยาว พิเศษ ทำให้ เส้น โครงร่าง ของ คน เรียว ยาว'}\n",
      "{'id': 3, 'src': '对 敌方 所有 目标 造成 魔法 伤害 ， 目标 低于 5 人时 每 减少 1 个 ， 伤害 增加 10% ， 同时 暴击 概率 降低 22% ， 持续 2 回合 ； 如果 目标 是 战士 职业 ， 驱散 其 所有 增益 状态 。', 'tgt': 'ทำ M . DMG ใส่ ศัตรู ทั้งหมด เมื่อ เป้าหมาย น้อยกว่า 5 คน ทุกครั้งที่ ลด 1 คน DMG เพิ่ม 10 % ขณะเดียวกัน อัตรา CRIT ลด 22 % ต่อเนื่อง 2 รอบ ； หาก เป้าหมาย เป็น นักรบ ขับไล่ บัฟ ทั้งหมด ของ เป้าหมาย'}\n",
      "{'id': 4, 'src': '烹@@ 煮 和 准备 美@@ 食', 'tgt': 'ปรุง@@ อาหาร และ เตรียม อาหาร'}\n"
     ]
    }
   ],
   "source": [
    "# 打印5条valid\n",
    "for data in dataset_valid[:5]:\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens转ids，组batch\n",
    "train_loader = prep_loader(conf, dataset_train, mode=\"train\" ,multi_process=False)\n",
    "valid_loader = prep_loader(conf, dataset_valid, mode=\"dev\",multi_process=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples_id:[312] , src_tokens:[312, 6], prev_tokens:[312, 13], tgt_tokens:[312, 13, 1]\n",
      "samples_id:[240] , src_tokens:[240, 8], prev_tokens:[240, 16], tgt_tokens:[240, 16, 1]\n",
      "samples_id:[184] , src_tokens:[184, 13], prev_tokens:[184, 22], tgt_tokens:[184, 22, 1]\n",
      "samples_id:[120] , src_tokens:[120, 20], prev_tokens:[120, 31], tgt_tokens:[120, 31, 1]\n",
      "samples_id:[88] , src_tokens:[88, 36], prev_tokens:[88, 42], tgt_tokens:[88, 42, 1]\n",
      "samples_id:[48] , src_tokens:[48, 63], prev_tokens:[48, 77], tgt_tokens:[48, 77, 1]\n"
     ]
    }
   ],
   "source": [
    "for batch_idx,batch_data in enumerate(valid_loader):\n",
    "    samples_id, src_tokens, prev_tokens, tgt_tokens = batch_data\n",
    "    print(f\"samples_id:{samples_id.shape} , src_tokens:{src_tokens.shape}, prev_tokens:{prev_tokens.shape}, tgt_tokens:{tgt_tokens.shape}\")\n",
    "    if batch_idx>4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  三、模型训练\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 模型组网\n",
    "\n",
    "Transformer 是论文 [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf) 中提出的用以完成机器翻译（Machine Translation）等序列到序列（Seq2Seq）学习任务的一种全新网络结构，其完全使用注意力（Attention）机制来实现序列到序列的建模。\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/cc62a2c1124346c18cc65590705d15787b2c6bbefc2d4f50a1c3cce055d61cf5)\n",
    "\n",
    "\n",
    "本项目默认使用[transformer_base](https://github.com/MiuGod0126/PaddleSeq/blob/7932c3f34e9da25bad5d7bffb158f11e2c2b7d5d/paddleseq/models/transformer.py#L677)结构，若要换成transformer_big，请在配置文件yaml中修改model_name:\n",
    "\n",
    "```\n",
    "model:\n",
    "  model_name: transformer_big\n",
    "```\n",
    "除此之外，还可以在命令行中指定网络结构，如： \n",
    "```\n",
    "python paddleseq_cli/train.py -c examples/ikcest22/configs/zh_th.yaml --arch transformer_big\n",
    "```\n",
    "\n",
    "如果想要魔改网络结构和参数，请在源码中添加新的结构[transformer.py](https://github.com/MiuGod0126/PaddleSeq/blob/7932c3f34e9da25bad5d7bffb158f11e2c2b7d5d/paddleseq/models/transformer.py)，如：\n",
    "```\n",
    "def transformer_big(is_test=False, pretrained_path=None, **kwargs):\n",
    "    for cfg in cfgs: assert cfg in kwargs, f'missing argument:{cfg}'\n",
    "    model_args = dict(encoder_layers=6,\n",
    "                      decoder_layers=6,\n",
    "                      d_model=1024,\n",
    "                      nheads=16,\n",
    "                      dim_feedforward=4096,\n",
    "                      **kwargs)\n",
    "    model_args = base_architecture(model_args)\n",
    "    model = _create_transformer('transformer_big', is_test, pretrained_path, model_args)\n",
    "    return model\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 模型加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0930 13:46:02.423334   140 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2\n",
      "W0930 13:46:02.428484   140 gpu_resources.cc:91] device: 0, cuDNN Version: 8.2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN model transformer_base created!\n",
      "Transformer(\n",
      "  (encoder): Encoder(\n",
      "    (layers): LayerList(\n",
      "      (0): EncoderLayer(\n",
      "        (self_attn): MultiHeadAttentionWithInit(\n",
      "          (q_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "          (k_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "        )\n",
      "        (norm1): LayerNorm(normalized_shape=[512], epsilon=1e-05)\n",
      "        (norm2): LayerNorm(normalized_shape=[512], epsilon=1e-05)\n",
      "        (dropout1): Dropout(p=0.3, axis=None, mode=upscale_in_train)\n",
      "        (dropout2): Dropout(p=0.3, axis=None, mode=upscale_in_train)\n",
      "        (mlp): Mlp(\n",
      "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
      "          (linear1): Linear(in_features=512, out_features=2048, dtype=float32)\n",
      "          (linear2): Linear(in_features=2048, out_features=512, dtype=float32)\n",
      "        )\n",
      "      )\n",
      "      (1): EncoderLayer(\n",
      "        (self_attn): MultiHeadAttentionWithInit(\n",
      "          (q_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "          (k_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "        )\n",
      "        (norm1): LayerNorm(normalized_shape=[512], epsilon=1e-05)\n",
      "        (norm2): LayerNorm(normalized_shape=[512], epsilon=1e-05)\n",
      "        (dropout1): Dropout(p=0.3, axis=None, mode=upscale_in_train)\n",
      "        (dropout2): Dropout(p=0.3, axis=None, mode=upscale_in_train)\n",
      "        (mlp): Mlp(\n",
      "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
      "          (linear1): Linear(in_features=512, out_features=2048, dtype=float32)\n",
      "          (linear2): Linear(in_features=2048, out_features=512, dtype=float32)\n",
      "        )\n",
      "      )\n",
      "      (2): EncoderLayer(\n",
      "        (self_attn): MultiHeadAttentionWithInit(\n",
      "          (q_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "          (k_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "        )\n",
      "        (norm1): LayerNorm(normalized_shape=[512], epsilon=1e-05)\n",
      "        (norm2): LayerNorm(normalized_shape=[512], epsilon=1e-05)\n",
      "        (dropout1): Dropout(p=0.3, axis=None, mode=upscale_in_train)\n",
      "        (dropout2): Dropout(p=0.3, axis=None, mode=upscale_in_train)\n",
      "        (mlp): Mlp(\n",
      "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
      "          (linear1): Linear(in_features=512, out_features=2048, dtype=float32)\n",
      "          (linear2): Linear(in_features=2048, out_features=512, dtype=float32)\n",
      "        )\n",
      "      )\n",
      "      (3): EncoderLayer(\n",
      "        (self_attn): MultiHeadAttentionWithInit(\n",
      "          (q_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "          (k_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "        )\n",
      "        (norm1): LayerNorm(normalized_shape=[512], epsilon=1e-05)\n",
      "        (norm2): LayerNorm(normalized_shape=[512], epsilon=1e-05)\n",
      "        (dropout1): Dropout(p=0.3, axis=None, mode=upscale_in_train)\n",
      "        (dropout2): Dropout(p=0.3, axis=None, mode=upscale_in_train)\n",
      "        (mlp): Mlp(\n",
      "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
      "          (linear1): Linear(in_features=512, out_features=2048, dtype=float32)\n",
      "          (linear2): Linear(in_features=2048, out_features=512, dtype=float32)\n",
      "        )\n",
      "      )\n",
      "      (4): EncoderLayer(\n",
      "        (self_attn): MultiHeadAttentionWithInit(\n",
      "          (q_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "          (k_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "        )\n",
      "        (norm1): LayerNorm(normalized_shape=[512], epsilon=1e-05)\n",
      "        (norm2): LayerNorm(normalized_shape=[512], epsilon=1e-05)\n",
      "        (dropout1): Dropout(p=0.3, axis=None, mode=upscale_in_train)\n",
      "        (dropout2): Dropout(p=0.3, axis=None, mode=upscale_in_train)\n",
      "        (mlp): Mlp(\n",
      "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
      "          (linear1): Linear(in_features=512, out_features=2048, dtype=float32)\n",
      "          (linear2): Linear(in_features=2048, out_features=512, dtype=float32)\n",
      "        )\n",
      "      )\n",
      "      (5): EncoderLayer(\n",
      "        (self_attn): MultiHeadAttentionWithInit(\n",
      "          (q_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "          (k_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "        )\n",
      "        (norm1): LayerNorm(normalized_shape=[512], epsilon=1e-05)\n",
      "        (norm2): LayerNorm(normalized_shape=[512], epsilon=1e-05)\n",
      "        (dropout1): Dropout(p=0.3, axis=None, mode=upscale_in_train)\n",
      "        (dropout2): Dropout(p=0.3, axis=None, mode=upscale_in_train)\n",
      "        (mlp): Mlp(\n",
      "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
      "          (linear1): Linear(in_features=512, out_features=2048, dtype=float32)\n",
      "          (linear2): Linear(in_features=2048, out_features=512, dtype=float32)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (embed_tokens): Embedding(24048, 512, sparse=False)\n",
      "    (embed_positions): PositionalEmbeddingLeanable(\n",
      "      (pos_encoder): Embedding(1026, 512, sparse=False)\n",
      "    )\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (layers): LayerList(\n",
      "      (0): DecoderLayer(\n",
      "        (self_attn): MultiHeadAttentionWithInit(\n",
      "          (q_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "          (k_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "        )\n",
      "        (cross_attn): MultiHeadAttentionWithInit(\n",
      "          (q_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "          (k_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "        )\n",
      "        (norm1): LayerNorm(normalized_shape=[512], epsilon=1e-05)\n",
      "        (norm2): LayerNorm(normalized_shape=[512], epsilon=1e-05)\n",
      "        (norm3): LayerNorm(normalized_shape=[512], epsilon=1e-05)\n",
      "        (dropout1): Dropout(p=0.3, axis=None, mode=upscale_in_train)\n",
      "        (dropout2): Dropout(p=0.3, axis=None, mode=upscale_in_train)\n",
      "        (dropout3): Dropout(p=0.3, axis=None, mode=upscale_in_train)\n",
      "        (mlp): Mlp(\n",
      "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
      "          (linear1): Linear(in_features=512, out_features=2048, dtype=float32)\n",
      "          (linear2): Linear(in_features=2048, out_features=512, dtype=float32)\n",
      "        )\n",
      "      )\n",
      "      (1): DecoderLayer(\n",
      "        (self_attn): MultiHeadAttentionWithInit(\n",
      "          (q_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "          (k_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "        )\n",
      "        (cross_attn): MultiHeadAttentionWithInit(\n",
      "          (q_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "          (k_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "        )\n",
      "        (norm1): LayerNorm(normalized_shape=[512], epsilon=1e-05)\n",
      "        (norm2): LayerNorm(normalized_shape=[512], epsilon=1e-05)\n",
      "        (norm3): LayerNorm(normalized_shape=[512], epsilon=1e-05)\n",
      "        (dropout1): Dropout(p=0.3, axis=None, mode=upscale_in_train)\n",
      "        (dropout2): Dropout(p=0.3, axis=None, mode=upscale_in_train)\n",
      "        (dropout3): Dropout(p=0.3, axis=None, mode=upscale_in_train)\n",
      "        (mlp): Mlp(\n",
      "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
      "          (linear1): Linear(in_features=512, out_features=2048, dtype=float32)\n",
      "          (linear2): Linear(in_features=2048, out_features=512, dtype=float32)\n",
      "        )\n",
      "      )\n",
      "      (2): DecoderLayer(\n",
      "        (self_attn): MultiHeadAttentionWithInit(\n",
      "          (q_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "          (k_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "        )\n",
      "        (cross_attn): MultiHeadAttentionWithInit(\n",
      "          (q_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "          (k_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "        )\n",
      "        (norm1): LayerNorm(normalized_shape=[512], epsilon=1e-05)\n",
      "        (norm2): LayerNorm(normalized_shape=[512], epsilon=1e-05)\n",
      "        (norm3): LayerNorm(normalized_shape=[512], epsilon=1e-05)\n",
      "        (dropout1): Dropout(p=0.3, axis=None, mode=upscale_in_train)\n",
      "        (dropout2): Dropout(p=0.3, axis=None, mode=upscale_in_train)\n",
      "        (dropout3): Dropout(p=0.3, axis=None, mode=upscale_in_train)\n",
      "        (mlp): Mlp(\n",
      "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
      "          (linear1): Linear(in_features=512, out_features=2048, dtype=float32)\n",
      "          (linear2): Linear(in_features=2048, out_features=512, dtype=float32)\n",
      "        )\n",
      "      )\n",
      "      (3): DecoderLayer(\n",
      "        (self_attn): MultiHeadAttentionWithInit(\n",
      "          (q_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "          (k_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "        )\n",
      "        (cross_attn): MultiHeadAttentionWithInit(\n",
      "          (q_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "          (k_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "        )\n",
      "        (norm1): LayerNorm(normalized_shape=[512], epsilon=1e-05)\n",
      "        (norm2): LayerNorm(normalized_shape=[512], epsilon=1e-05)\n",
      "        (norm3): LayerNorm(normalized_shape=[512], epsilon=1e-05)\n",
      "        (dropout1): Dropout(p=0.3, axis=None, mode=upscale_in_train)\n",
      "        (dropout2): Dropout(p=0.3, axis=None, mode=upscale_in_train)\n",
      "        (dropout3): Dropout(p=0.3, axis=None, mode=upscale_in_train)\n",
      "        (mlp): Mlp(\n",
      "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
      "          (linear1): Linear(in_features=512, out_features=2048, dtype=float32)\n",
      "          (linear2): Linear(in_features=2048, out_features=512, dtype=float32)\n",
      "        )\n",
      "      )\n",
      "      (4): DecoderLayer(\n",
      "        (self_attn): MultiHeadAttentionWithInit(\n",
      "          (q_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "          (k_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "        )\n",
      "        (cross_attn): MultiHeadAttentionWithInit(\n",
      "          (q_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "          (k_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "        )\n",
      "        (norm1): LayerNorm(normalized_shape=[512], epsilon=1e-05)\n",
      "        (norm2): LayerNorm(normalized_shape=[512], epsilon=1e-05)\n",
      "        (norm3): LayerNorm(normalized_shape=[512], epsilon=1e-05)\n",
      "        (dropout1): Dropout(p=0.3, axis=None, mode=upscale_in_train)\n",
      "        (dropout2): Dropout(p=0.3, axis=None, mode=upscale_in_train)\n",
      "        (dropout3): Dropout(p=0.3, axis=None, mode=upscale_in_train)\n",
      "        (mlp): Mlp(\n",
      "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
      "          (linear1): Linear(in_features=512, out_features=2048, dtype=float32)\n",
      "          (linear2): Linear(in_features=2048, out_features=512, dtype=float32)\n",
      "        )\n",
      "      )\n",
      "      (5): DecoderLayer(\n",
      "        (self_attn): MultiHeadAttentionWithInit(\n",
      "          (q_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "          (k_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "        )\n",
      "        (cross_attn): MultiHeadAttentionWithInit(\n",
      "          (q_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "          (k_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "        )\n",
      "        (norm1): LayerNorm(normalized_shape=[512], epsilon=1e-05)\n",
      "        (norm2): LayerNorm(normalized_shape=[512], epsilon=1e-05)\n",
      "        (norm3): LayerNorm(normalized_shape=[512], epsilon=1e-05)\n",
      "        (dropout1): Dropout(p=0.3, axis=None, mode=upscale_in_train)\n",
      "        (dropout2): Dropout(p=0.3, axis=None, mode=upscale_in_train)\n",
      "        (dropout3): Dropout(p=0.3, axis=None, mode=upscale_in_train)\n",
      "        (mlp): Mlp(\n",
      "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
      "          (linear1): Linear(in_features=512, out_features=2048, dtype=float32)\n",
      "          (linear2): Linear(in_features=2048, out_features=512, dtype=float32)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (embed_tokens): Embedding(16194, 512, sparse=False)\n",
      "    (embed_positions): PositionalEmbeddingLeanable(\n",
      "      (pos_encoder): Embedding(1026, 512, sparse=False)\n",
      "    )\n",
      "  )\n",
      "  (output_projection): Linear(in_features=512, out_features=16194, dtype=float32)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from paddleseq.models import build_model\n",
    "model = build_model(conf, is_test=False)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "运行耗时: 4秒585毫秒\n",
    "TRAIN model transformer_base created!\n",
    "Transformer(\n",
    "  (encoder): Encoder(\n",
    "    (layers): LayerList(\n",
    "      (0): EncoderLayer(\n",
    "        (self_attn): MultiHeadAttentionWithInit(\n",
    "          (q_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
    "          (k_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
    "          (v_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
    "          (out_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
    "        )\n",
    "        (norm1): LayerNorm(normalized_shape=[512], epsilon=1e-05)\n",
    "        (norm2): LayerNorm(normalized_shape=[512], epsilon=1e-05)\n",
    "        (dropout1): Dropout(p=0.3, axis=None, mode=upscale_in_train)\n",
    "        (dropout2): Dropout(p=0.3, axis=None, mode=upscale_in_train)\n",
    "        (mlp): Mlp(\n",
    "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
    "          (linear1): Linear(in_features=512, out_features=2048, dtype=float32)\n",
    "          (linear2): Linear(in_features=2048, out_features=512, dtype=float32)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 除此之外，还可以加载权重目录中的model.yaml （ckpt_path需要修改成自己的权重目录）\n",
    "# from paddleseq.models import build_model\n",
    "# ckpt_path=\"output/ckpt_zhth/epoch_final/\"\n",
    "# model = build_model(ckpt_path, is_test=False)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "TRAIN model transformer_base created!\n",
    "Pretrained weight load from:output/ckpt_zhth/epoch_final/model.pdparams!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 中泰训练\n",
    "\n",
    "本节以中泰为例，简介下paddleseq的训练命令。\n",
    "\n",
    "**注：如果想快点提交，可以无脑run 3.4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fr_zh.yaml  ru_zh.yaml\tth_zh.yaml  zh_fr.yaml\tzh_ru.yaml  zh_th.yaml\r\n"
     ]
    }
   ],
   "source": [
    "# 查看配置文件\n",
    "!ls examples/ikcest22/configs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 2022-09-30 13:53:39,426 cloud_utils.py:122] get cluster from args:job_server:None pods:['rank:0 id:None addr:127.0.0.1 port:None visible_gpu:[] trainers:[\"gpu:[\\'0\\'] endpoint:127.0.0.1:34625 rank:0\", \"gpu:[\\'1\\'] endpoint:127.0.0.1:47373 rank:1\", \"gpu:[\\'2\\'] endpoint:127.0.0.1:42955 rank:2\", \"gpu:[\\'3\\'] endpoint:127.0.0.1:33581 rank:3\"]'] job_stage_flag:None hdfs:None\n",
      "server not ready, wait 3 sec to retry...\n",
      "not ready endpoints:['127.0.0.1:47373', '127.0.0.1:42955', '127.0.0.1:33581']\n",
      "I0930 13:53:43.859783 29151 gen_comm_id_helper.cc:205] Server listening on: 127.0.0.1:47373 successful.\n",
      "server not ready, wait 3 sec to retry...\n",
      "not ready endpoints:['127.0.0.1:42955', '127.0.0.1:33581']\n",
      "I0930 13:53:46.049103 29164 gen_comm_id_helper.cc:205] Server listening on: 127.0.0.1:42955 successful.\n",
      "server not ready, wait 3 sec to retry...\n",
      "not ready endpoints:['127.0.0.1:33581']\n",
      "I0930 13:53:48.199968 29177 gen_comm_id_helper.cc:205] Server listening on: 127.0.0.1:33581 successful.\n",
      "I0930 13:53:50.700968 29138 nccl_context.cc:83] init nccl context nranks: 4 local rank: 0 gpu id: 0 ring id: 0\n",
      "I0930 13:53:50.700959 29151 nccl_context.cc:83] init nccl context nranks: 4 local rank: 1 gpu id: 1 ring id: 0\n",
      "I0930 13:53:50.700973 29164 nccl_context.cc:83] init nccl context nranks: 4 local rank: 2 gpu id: 2 ring id: 0\n",
      "I0930 13:53:50.701015 29177 nccl_context.cc:83] init nccl context nranks: 4 local rank: 3 gpu id: 3 ring id: 0\n",
      "W0930 13:53:51.433337 29151 gpu_resources.cc:61] Please NOTE: device: 1, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2\n",
      "W0930 13:53:51.433573 29138 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2\n",
      "W0930 13:53:51.434171 29177 gpu_resources.cc:61] Please NOTE: device: 3, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2\n",
      "W0930 13:53:51.434659 29164 gpu_resources.cc:61] Please NOTE: device: 2, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2\n",
      "W0930 13:53:51.436904 29151 gpu_resources.cc:91] device: 1, cuDNN Version: 8.2.\n",
      "W0930 13:53:51.437069 29138 gpu_resources.cc:91] device: 0, cuDNN Version: 8.2.\n",
      "W0930 13:53:51.437562 29177 gpu_resources.cc:91] device: 3, cuDNN Version: 8.2.\n",
      "W0930 13:53:51.437844 29164 gpu_resources.cc:91] device: 2, cuDNN Version: 8.2.\n",
      "2022-09-30 13:53:52,351 | transformer_base_rank3: ----- world_size = 4, local_rank = 3\n",
      "2022-09-30 13:53:52,362 | transformer_base_rank1: ----- world_size = 4, local_rank = 1\n",
      "2022-09-30 13:53:52,366 | transformer_base_rank2: ----- world_size = 4, local_rank = 2\n",
      "2022-09-30 13:53:52,375 | transformer_base_rank0: ----- world_size = 4, local_rank = 0\n",
      "2022-09-30 13:53:54,116 | transformer_base_rank2: ----- Total of train set:99000 ,train batch: 102 [single gpu]\n",
      "2022-09-30 13:53:54,130 | transformer_base_rank3: ----- Total of train set:99000 ,train batch: 102 [single gpu]\n",
      "2022-09-30 13:53:54,144 | transformer_base_rank1: ----- Total of train set:99000 ,train batch: 102 [single gpu]\n",
      "2022-09-30 13:53:54,152 | transformer_base_rank2: ----- Total of valid set:1000 ,valid batch: 2 [single gpu]\n",
      "2022-09-30 13:53:54,152 | transformer_base_rank2: Load data cost 1.7849805355072021 seconds.\n",
      "2022-09-30 13:53:54,161 | transformer_base_rank0: ----- Total of train set:99000 ,train batch: 102 [single gpu]\n",
      "2022-09-30 13:53:54,168 | transformer_base_rank3: ----- Total of valid set:1000 ,valid batch: 2 [single gpu]\n",
      "2022-09-30 13:53:54,168 | transformer_base_rank3: Load data cost 1.8155360221862793 seconds.\n",
      "2022-09-30 13:53:54,181 | transformer_base_rank1: ----- Total of valid set:1000 ,valid batch: 2 [single gpu]\n",
      "2022-09-30 13:53:54,182 | transformer_base_rank1: Load data cost 1.818664312362671 seconds.\n",
      "2022-09-30 13:53:54,197 | transformer_base_rank0: ----- Total of valid set:1000 ,valid batch: 2 [single gpu]\n",
      "2022-09-30 13:53:54,197 | transformer_base_rank0: Load data cost 1.821028232574463 seconds.\n",
      "2022-09-30 13:53:54,198 | transformer_base_rank0: configs:\n",
      "SAVE: output\n",
      "data:\n",
      "  has_target: True\n",
      "  lang_embed: False\n",
      "  language_token: ['<de>', '<en>']\n",
      "  lazy_load: False\n",
      "  pad_factor: 8\n",
      "  pad_vocab: False\n",
      "  special_token: ['<s>', '<pad>', '</s>', '<unk>']\n",
      "  src_bpe_path: datasets/bpe/zh_th/code.zh\n",
      "  src_lang: zh\n",
      "  test_pref: datasets/bpe/zh_th/test.zh_th\n",
      "  tgt_bpe_path: datasets/bpe/zh_th/code.th\n",
      "  tgt_lang: th\n",
      "  train_pref: datasets/bpe/zh_th/train\n",
      "  truecase_path: None\n",
      "  use_binary: False\n",
      "  use_moses_bpe: False\n",
      "  valid_pref: datasets/bpe/zh_th/valid\n",
      "  vocab_pref: datasets/bpe/zh_th/vocab\n",
      "  with_tag: False\n",
      "eval: False\n",
      "eval_beam: False\n",
      "generate:\n",
      "  beam_size: 5\n",
      "  detokenize: False\n",
      "  generate_path: generate.txt\n",
      "  infer_bsz: 256\n",
      "  max_out_len: 200\n",
      "  max_sentences: None\n",
      "  n_best: 1\n",
      "  search_strategy: beam\n",
      "  sorted_path: result.txt\n",
      "learning_strategy:\n",
      "  clip_norm: 0\n",
      "  clip_type: local\n",
      "  label_smooth_eps: 0.1\n",
      "  learning_rate: 5e-4\n",
      "  min_lr: -1\n",
      "  optim: adam\n",
      "  optimizer:\n",
      "    adam:\n",
      "      beta1: 0.9\n",
      "      beta2: 0.98\n",
      "    nag:\n",
      "      momentum: 0.99\n",
      "      use_nesterov: True\n",
      "  reset_lr: False\n",
      "  sched: inverse_sqrt\n",
      "  scheduler:\n",
      "    cosine:\n",
      "      t_max: 10\n",
      "    inverse_sqrt:\n",
      "      warmup_init_lr: 1e-7\n",
      "    knee:\n",
      "      explore_epochs: 40\n",
      "      warmup_init_lr: 1e-7\n",
      "    linear:\n",
      "      warm_steps: 4000\n",
      "    noamdecay:\n",
      "      d_model: 512\n",
      "    plateau:\n",
      "      force_anneal: 50\n",
      "      lr_shrink: 0.1\n",
      "      patience: 1\n",
      "  warm_steps: 4000\n",
      "  weight_decay: 1e-4\n",
      "model:\n",
      "  bos_idx: 0\n",
      "  dmodel: 512\n",
      "  dropout: 0.3\n",
      "  eos_idx: 2\n",
      "  init_from_params: \n",
      "  max_length: 1024\n",
      "  min_length: 0\n",
      "  model_name: transformer_base\n",
      "  pad_idx: 1\n",
      "  save_model: ckpt_zhth\n",
      "  src_vocab_size: 24048\n",
      "  tgt_vocab_size: 16194\n",
      "  unk_idx: 3\n",
      "ngpus: 4\n",
      "seed: 1\n",
      "train:\n",
      "  amp: False\n",
      "  amp_scale_window: False\n",
      "  batch_size_factor: 8\n",
      "  eval_beam: False\n",
      "  fp16_init_scale: 128\n",
      "  growth_interval: 128\n",
      "  last_epoch: 0\n",
      "  last_step: 0\n",
      "  log_steps: 100\n",
      "  max_epoch: 10\n",
      "  max_sentences: None\n",
      "  max_tokens: 4096\n",
      "  max_update: -1\n",
      "  num_workers: 1\n",
      "  pool_size: 200000\n",
      "  report_bleu: True\n",
      "  resume: \n",
      "  save_epoch: 10\n",
      "  save_step: 0\n",
      "  shuffle: True\n",
      "  shuffle_batch: True\n",
      "  sort_type: pool\n",
      "  stop_patience: -1\n",
      "  train_data_size: -1\n",
      "  update_freq: 4\n",
      "  use_gpu: True\n",
      "TRAIN model transformer_base created!\n",
      "TRAIN model transformer_base created!\n",
      "TRAIN model transformer_base created!\n",
      "TRAIN model transformer_base created!\n",
      "2022-09-30 13:53:55,064 | transformer_base_rank0: model:\n",
      "DataParallel(\n",
      "  (_layers): Transformer(\n",
      "    (encoder): Encoder(\n",
      "      (layers): LayerList(\n",
      "        (0): EncoderLayer(\n",
      "          (self_attn): MultiHeadAttentionWithInit(\n",
      "            (q_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "            (k_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "          )\n",
      "          (norm1): LayerNorm(normalized_shape=[512], epsilon=1e-05)\n",
      "          (norm2): LayerNorm(normalized_shape=[512], epsilon=1e-05)\n",
      "          (dropout1): Dropout(p=0.3, axis=None, mode=upscale_in_train)\n",
      "          (dropout2): Dropout(p=0.3, axis=None, mode=upscale_in_train)\n",
      "          (mlp): Mlp(\n",
      "            (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
      "            (linear1): Linear(in_features=512, out_features=2048, dtype=float32)\n",
      "            (linear2): Linear(in_features=2048, out_features=512, dtype=float32)\n",
      "          )\n",
      "        )\n",
      "        (1): EncoderLayer(\n",
      "          (self_attn): MultiHeadAttentionWithInit(\n",
      "            (q_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "            (k_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "          )\n",
      "          (norm1): LayerNorm(normalized_shape=[512], epsilon=1e-05)\n",
      "          (norm2): LayerNorm(normalized_shape=[512], epsilon=1e-05)\n",
      "          (dropout1): Dropout(p=0.3, axis=None, mode=upscale_in_train)\n",
      "          (dropout2): Dropout(p=0.3, axis=None, mode=upscale_in_train)\n",
      "          (mlp): Mlp(\n",
      "            (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
      "            (linear1): Linear(in_features=512, out_features=2048, dtype=float32)\n",
      "            (linear2): Linear(in_features=2048, out_features=512, dtype=float32)\n",
      "          )\n",
      "        )\n",
      "        (2): EncoderLayer(\n",
      "          (self_attn): MultiHeadAttentionWithInit(\n",
      "            (q_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "            (k_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "          )\n",
      "          (norm1): LayerNorm(normalized_shape=[512], epsilon=1e-05)\n",
      "          (norm2): LayerNorm(normalized_shape=[512], epsilon=1e-05)\n",
      "          (dropout1): Dropout(p=0.3, axis=None, mode=upscale_in_train)\n",
      "          (dropout2): Dropout(p=0.3, axis=None, mode=upscale_in_train)\n",
      "          (mlp): Mlp(\n",
      "            (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
      "            (linear1): Linear(in_features=512, out_features=2048, dtype=float32)\n",
      "            (linear2): Linear(in_features=2048, out_features=512, dtype=float32)\n",
      "          )\n",
      "        )\n",
      "        (3): EncoderLayer(\n",
      "          (self_attn): MultiHeadAttentionWithInit(\n",
      "            (q_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "            (k_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "          )\n",
      "          (norm1): LayerNorm(normalized_shape=[512], epsilon=1e-05)\n",
      "          (norm2): LayerNorm(normalized_shape=[512], epsilon=1e-05)\n",
      "          (dropout1): Dropout(p=0.3, axis=None, mode=upscale_in_train)\n",
      "          (dropout2): Dropout(p=0.3, axis=None, mode=upscale_in_train)\n",
      "          (mlp): Mlp(\n",
      "            (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
      "            (linear1): Linear(in_features=512, out_features=2048, dtype=float32)\n",
      "            (linear2): Linear(in_features=2048, out_features=512, dtype=float32)\n",
      "          )\n",
      "        )\n",
      "        (4): EncoderLayer(\n",
      "          (self_attn): MultiHeadAttentionWithInit(\n",
      "            (q_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "            (k_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "          )\n",
      "          (norm1): LayerNorm(normalized_shape=[512], epsilon=1e-05)\n",
      "          (norm2): LayerNorm(normalized_shape=[512], epsilon=1e-05)\n",
      "          (dropout1): Dropout(p=0.3, axis=None, mode=upscale_in_train)\n",
      "          (dropout2): Dropout(p=0.3, axis=None, mode=upscale_in_train)\n",
      "          (mlp): Mlp(\n",
      "            (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
      "            (linear1): Linear(in_features=512, out_features=2048, dtype=float32)\n",
      "            (linear2): Linear(in_features=2048, out_features=512, dtype=float32)\n",
      "          )\n",
      "        )\n",
      "        (5): EncoderLayer(\n",
      "          (self_attn): MultiHeadAttentionWithInit(\n",
      "            (q_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "            (k_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "          )\n",
      "          (norm1): LayerNorm(normalized_shape=[512], epsilon=1e-05)\n",
      "          (norm2): LayerNorm(normalized_shape=[512], epsilon=1e-05)\n",
      "          (dropout1): Dropout(p=0.3, axis=None, mode=upscale_in_train)\n",
      "          (dropout2): Dropout(p=0.3, axis=None, mode=upscale_in_train)\n",
      "          (mlp): Mlp(\n",
      "            (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
      "            (linear1): Linear(in_features=512, out_features=2048, dtype=float32)\n",
      "            (linear2): Linear(in_features=2048, out_features=512, dtype=float32)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (embed_tokens): Embedding(24048, 512, sparse=False)\n",
      "      (embed_positions): PositionalEmbeddingLeanable(\n",
      "        (pos_encoder): Embedding(1026, 512, sparse=False)\n",
      "      )\n",
      "    )\n",
      "    (decoder): Decoder(\n",
      "      (layers): LayerList(\n",
      "        (0): DecoderLayer(\n",
      "          (self_attn): MultiHeadAttentionWithInit(\n",
      "            (q_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "            (k_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "          )\n",
      "          (cross_attn): MultiHeadAttentionWithInit(\n",
      "            (q_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "            (k_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "          )\n",
      "          (norm1): LayerNorm(normalized_shape=[512], epsilon=1e-05)\n",
      "          (norm2): LayerNorm(normalized_shape=[512], epsilon=1e-05)\n",
      "          (norm3): LayerNorm(normalized_shape=[512], epsilon=1e-05)\n",
      "          (dropout1): Dropout(p=0.3, axis=None, mode=upscale_in_train)\n",
      "          (dropout2): Dropout(p=0.3, axis=None, mode=upscale_in_train)\n",
      "          (dropout3): Dropout(p=0.3, axis=None, mode=upscale_in_train)\n",
      "          (mlp): Mlp(\n",
      "            (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
      "            (linear1): Linear(in_features=512, out_features=2048, dtype=float32)\n",
      "            (linear2): Linear(in_features=2048, out_features=512, dtype=float32)\n",
      "          )\n",
      "        )\n",
      "        (1): DecoderLayer(\n",
      "          (self_attn): MultiHeadAttentionWithInit(\n",
      "            (q_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "            (k_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "          )\n",
      "          (cross_attn): MultiHeadAttentionWithInit(\n",
      "            (q_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "            (k_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "          )\n",
      "          (norm1): LayerNorm(normalized_shape=[512], epsilon=1e-05)\n",
      "          (norm2): LayerNorm(normalized_shape=[512], epsilon=1e-05)\n",
      "          (norm3): LayerNorm(normalized_shape=[512], epsilon=1e-05)\n",
      "          (dropout1): Dropout(p=0.3, axis=None, mode=upscale_in_train)\n",
      "          (dropout2): Dropout(p=0.3, axis=None, mode=upscale_in_train)\n",
      "          (dropout3): Dropout(p=0.3, axis=None, mode=upscale_in_train)\n",
      "          (mlp): Mlp(\n",
      "            (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
      "            (linear1): Linear(in_features=512, out_features=2048, dtype=float32)\n",
      "            (linear2): Linear(in_features=2048, out_features=512, dtype=float32)\n",
      "          )\n",
      "        )\n",
      "        (2): DecoderLayer(\n",
      "          (self_attn): MultiHeadAttentionWithInit(\n",
      "            (q_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "            (k_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "          )\n",
      "          (cross_attn): MultiHeadAttentionWithInit(\n",
      "            (q_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "            (k_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "          )\n",
      "          (norm1): LayerNorm(normalized_shape=[512], epsilon=1e-05)\n",
      "          (norm2): LayerNorm(normalized_shape=[512], epsilon=1e-05)\n",
      "          (norm3): LayerNorm(normalized_shape=[512], epsilon=1e-05)\n",
      "          (dropout1): Dropout(p=0.3, axis=None, mode=upscale_in_train)\n",
      "          (dropout2): Dropout(p=0.3, axis=None, mode=upscale_in_train)\n",
      "          (dropout3): Dropout(p=0.3, axis=None, mode=upscale_in_train)\n",
      "          (mlp): Mlp(\n",
      "            (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
      "            (linear1): Linear(in_features=512, out_features=2048, dtype=float32)\n",
      "            (linear2): Linear(in_features=2048, out_features=512, dtype=float32)\n",
      "          )\n",
      "        )\n",
      "        (3): DecoderLayer(\n",
      "          (self_attn): MultiHeadAttentionWithInit(\n",
      "            (q_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "            (k_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "          )\n",
      "          (cross_attn): MultiHeadAttentionWithInit(\n",
      "            (q_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "            (k_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "          )\n",
      "          (norm1): LayerNorm(normalized_shape=[512], epsilon=1e-05)\n",
      "          (norm2): LayerNorm(normalized_shape=[512], epsilon=1e-05)\n",
      "          (norm3): LayerNorm(normalized_shape=[512], epsilon=1e-05)\n",
      "          (dropout1): Dropout(p=0.3, axis=None, mode=upscale_in_train)\n",
      "          (dropout2): Dropout(p=0.3, axis=None, mode=upscale_in_train)\n",
      "          (dropout3): Dropout(p=0.3, axis=None, mode=upscale_in_train)\n",
      "          (mlp): Mlp(\n",
      "            (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
      "            (linear1): Linear(in_features=512, out_features=2048, dtype=float32)\n",
      "            (linear2): Linear(in_features=2048, out_features=512, dtype=float32)\n",
      "          )\n",
      "        )\n",
      "        (4): DecoderLayer(\n",
      "          (self_attn): MultiHeadAttentionWithInit(\n",
      "            (q_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "            (k_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "          )\n",
      "          (cross_attn): MultiHeadAttentionWithInit(\n",
      "            (q_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "            (k_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "          )\n",
      "          (norm1): LayerNorm(normalized_shape=[512], epsilon=1e-05)\n",
      "          (norm2): LayerNorm(normalized_shape=[512], epsilon=1e-05)\n",
      "          (norm3): LayerNorm(normalized_shape=[512], epsilon=1e-05)\n",
      "          (dropout1): Dropout(p=0.3, axis=None, mode=upscale_in_train)\n",
      "          (dropout2): Dropout(p=0.3, axis=None, mode=upscale_in_train)\n",
      "          (dropout3): Dropout(p=0.3, axis=None, mode=upscale_in_train)\n",
      "          (mlp): Mlp(\n",
      "            (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
      "            (linear1): Linear(in_features=512, out_features=2048, dtype=float32)\n",
      "            (linear2): Linear(in_features=2048, out_features=512, dtype=float32)\n",
      "          )\n",
      "        )\n",
      "        (5): DecoderLayer(\n",
      "          (self_attn): MultiHeadAttentionWithInit(\n",
      "            (q_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "            (k_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "          )\n",
      "          (cross_attn): MultiHeadAttentionWithInit(\n",
      "            (q_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "            (k_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, dtype=float32)\n",
      "          )\n",
      "          (norm1): LayerNorm(normalized_shape=[512], epsilon=1e-05)\n",
      "          (norm2): LayerNorm(normalized_shape=[512], epsilon=1e-05)\n",
      "          (norm3): LayerNorm(normalized_shape=[512], epsilon=1e-05)\n",
      "          (dropout1): Dropout(p=0.3, axis=None, mode=upscale_in_train)\n",
      "          (dropout2): Dropout(p=0.3, axis=None, mode=upscale_in_train)\n",
      "          (dropout3): Dropout(p=0.3, axis=None, mode=upscale_in_train)\n",
      "          (mlp): Mlp(\n",
      "            (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
      "            (linear1): Linear(in_features=512, out_features=2048, dtype=float32)\n",
      "            (linear2): Linear(in_features=2048, out_features=512, dtype=float32)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (embed_tokens): Embedding(16194, 512, sparse=False)\n",
      "      (embed_positions): PositionalEmbeddingLeanable(\n",
      "        (pos_encoder): Embedding(1026, 512, sparse=False)\n",
      "      )\n",
      "    )\n",
      "    (output_projection): Linear(in_features=512, out_features=16194, dtype=float32)\n",
      "  )\n",
      ")\n",
      "2022-09-30 13:53:55,066 | transformer_base_rank3: Now training epoch 1. LR=0.00000\n",
      "2022-09-30 13:53:55,066 | transformer_base_rank1: Now training epoch 1. LR=0.00000\n",
      "2022-09-30 13:53:55,066 | transformer_base_rank2: Now training epoch 1. LR=0.00000\n",
      "2022-09-30 13:53:55,066 | transformer_base_rank0: Now training epoch 1. LR=0.00000\n",
      "2022-09-30 13:54:15,477 | transformer_base_rank1: Train| epoch:[1/10], step:[100/100], speed:4.90 step/s, loss:14.182, nll_loss:14.122, ppl:17831.50, bsz:987.5, gnorm:8.960, num_updates:100, lr:0.000012348\n",
      "2022-09-30 13:54:15,477 | transformer_base_rank3: Train| epoch:[1/10], step:[100/100], speed:4.90 step/s, loss:14.185, nll_loss:14.125, ppl:17866.11, bsz:987.5, gnorm:8.960, num_updates:100, lr:0.000012348\n",
      "2022-09-30 13:54:15,477 | transformer_base_rank2: Train| epoch:[1/10], step:[100/100], speed:4.90 step/s, loss:14.183, nll_loss:14.123, ppl:17841.19, bsz:987.5, gnorm:8.960, num_updates:100, lr:0.000012348\n",
      "2022-09-30 13:54:15,477 | transformer_base_rank0: Train| epoch:[1/10], step:[100/100], speed:4.90 step/s, loss:14.182, nll_loss:14.122, ppl:17831.42, bsz:987.5, gnorm:8.960, num_updates:100, lr:0.000012348\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/multiprocessing/semaphore_tracker.py:144: UserWarning: semaphore_tracker: There appear to be 1 leaked semaphores to clean up at shutdown\n",
      "  len(cache))\n",
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]/opt/conda/envs/python35-paddle120-env/lib/python3.7/multiprocessing/semaphore_tracker.py:144: UserWarning: semaphore_tracker: There appear to be 1 leaked semaphores to clean up at shutdown\n",
      "  len(cache))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/multiprocessing/semaphore_tracker.py:144: UserWarning: semaphore_tracker: There appear to be 1 leaked semaphores to clean up at shutdown\n",
      "  len(cache))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/multiprocessing/semaphore_tracker.py:144: UserWarning: semaphore_tracker: There appear to be 1 leaked semaphores to clean up at shutdown\n",
      "  len(cache))\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:03<00:00,  2.27s/it]/opt/conda/envs/python35-paddle120-env/lib/python3.7/multiprocessing/semaphore_tracker.py:144: UserWarning: semaphore_tracker: There appear to be 1 leaked semaphores to clean up at shutdown\n",
      "  len(cache))\n",
      "\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/multiprocessing/semaphore_tracker.py:144: UserWarning: semaphore_tracker: There appear to be 1 leaked semaphores to clean up at shutdown\n",
      "  len(cache))\n",
      "\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/multiprocessing/semaphore_tracker.py:144: UserWarning: semaphore_tracker: There appear to be 1 leaked semaphores to clean up at shutdown\n",
      "  len(cache))\n",
      "\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/multiprocessing/semaphore_tracker.py:144: UserWarning: semaphore_tracker: There appear to be 1 leaked semaphores to clean up at shutdown\n",
      "  len(cache))\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"paddleseq_cli/train.py\", line 408, in <module>\n",
      "    main()\n",
      "  File \"paddleseq_cli/train.py\", line 404, in main\n",
      "    dist.spawn(main_worker, args=(conf, dataset_train, dataset_dev,), nprocs=conf.ngpus)\n",
      "  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/distributed/spawn.py\", line 565, in spawn\n",
      "    while not context.join():\n",
      "  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/distributed/spawn.py\", line 373, in join\n",
      "    self._throw_exception(error_index)\n",
      "  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/distributed/spawn.py\", line 391, in _throw_exception\n",
      "    raise Exception(msg)\n",
      "Exception: \n",
      "\n",
      "----------------------------------------------\n",
      "Process 2 terminated with the following error:\n",
      "----------------------------------------------\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/distributed/spawn.py\", line 322, in _func_wrapper\n",
      "    result = func(*args)\n",
      "  File \"/home/aistudio/PaddleSeq/paddleseq_cli/train.py\", line 351, in main_worker\n",
      "    val_loss, val_nll_loss, val_ppl, dev_bleu = validation(conf, dev_loader, model, criterion, logger)\n",
      "  File \"<decorator-gen-435>\", line 2, in validation\n",
      "  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/base.py\", line 354, in _decorate_function\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/aistudio/PaddleSeq/paddleseq_cli/validate.py\", line 88, in validation\n",
      "    dev_bleu = round(scorer.score() * 100,3)\n",
      "  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddlenlp/metrics/bleu.py\", line 231, in score\n",
      "    return self.accumulate()\n",
      "  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddlenlp/metrics/bleu.py\", line 226, in accumulate\n",
      "    bp = math.exp(min(1 - self.bp_r / float(self.bp_c), 0))\n",
      "ZeroDivisionError: float division by zero\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 训练中泰3epoch\n",
    "# !export PYTHONWARNINGS='ignore:semaphore_tracker:UserWarning'  # 可以忽略些warn信息\n",
    "!python paddleseq_cli/train.py -c examples/ikcest22/configs/zh_th.yaml --update-freq 4  --max-epoch 10 --ngpus 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "2022-09-04 18:39:24,058 | transformer_base_rank0: ----- Total of train set:99000 ,train batch: 407 [single gpu]\n",
    "2022-09-04 18:39:24,104 | transformer_base_rank0: ----- Total of valid set:1000 ,valid batch: 7 [single gpu]\n",
    "2022-09-04 18:39:24,105 | transformer_base_rank0: Load data cost 1.7559177875518799 seconds.\n",
    "2022-09-04 18:40:44,284 | transformer_base_rank0: Now training epoch 2. LR=0.00005\n",
    "2022-09-04 18:41:01,809 | transformer_base_rank0: Train| epoch:[2/3], step:[100/395], speed:5.71 step/s, loss:11.520, nll_loss:11.148, ppl:2269.24, bsz:1001.9, gnorm:3.339, num_updates:123, lr:0.000063212\n",
    "2022-09-04 18:41:15,831 | transformer_base_rank0: Train| epoch:[2/3], step:[200/391], speed:7.13 step/s, loss:11.307, nll_loss:10.899, ppl:1909.37, bsz:1011.2, gnorm:3.999, num_updates:147, lr:0.000075710\n",
    "2022-09-04 18:41:30,304 | transformer_base_rank0: Train| epoch:[2/3], step:[300/404], speed:6.91 step/s, loss:11.169, nll_loss:10.731, ppl:1700.19, bsz:979.0, gnorm:4.359, num_updates:176, lr:0.000088207\n",
    "2022-09-04 18:41:44,636 | transformer_base_rank0: Train| epoch:[2/3], step:[400/405], speed:6.98 step/s, loss:11.061, nll_loss:10.599, ppl:1551.27, bsz:977.0, gnorm:4.157, num_updates:201, lr:0.000100705\n",
    "100%|█████████████████████████████████████████████| 7/7 [00:07<00:00,  1.07it/s]\n",
    "2022-09-04 18:41:54,192 | transformer_base_rank0: Eval | Avg loss: 10.350 | nll_loss:9.716 | ppl: 900.017 | Eval | BLEU Score: 1.239\n",
    "current checkpoints: ['model_best_0.0', 'epoch_final', 'model_best_1.239']\n",
    "2022-09-04 18:42:01,859 | transformer_base_rank0: Epoch:[2] | Best Valid Bleu: [1.239] saved to output/ckpt_zhth/model_best_1.239!\n",
    "2022-09-04 18:42:01,859 | transformer_base_rank0: Now training epoch 3. LR=0.00010\n",
    "2022-09-04 18:42:18,927 | transformer_base_rank0: Train| epoch:[3/3], step:[100/402], speed:5.86 step/s, loss:10.527, nll_loss:9.950, ppl:989.44, bsz:983.4, gnorm:3.806, num_updates:226, lr:0.000114077\n",
    "2022-09-04 18:42:33,549 | transformer_base_rank0: Train| epoch:[3/3], step:[200/431], speed:6.84 step/s, loss:10.486, nll_loss:9.902, ppl:957.00, bsz:917.0, gnorm:3.867, num_updates:265, lr:0.000126575\n",
    "2022-09-04 18:42:48,090 | transformer_base_rank0: Train| epoch:[3/3], step:[300/416], speed:6.88 step/s, loss:10.350, nll_loss:9.747, ppl:859.50, bsz:951.3, gnorm:4.090, num_updates:283, lr:0.000139072\n",
    "2022-09-04 18:43:03,210 | transformer_base_rank0: Train| epoch:[3/3], step:[400/403], speed:6.61 step/s, loss:10.284, nll_loss:9.671, ppl:815.06, bsz:980.2, gnorm:4.266, num_updates:301, lr:0.000151570\n",
    "100%|█████████████████████████████████████████████| 7/7 [00:08<00:00,  1.09s/it]\n",
    "2022-09-04 18:43:16,024 | transformer_base_rank0: Epoch:[3] | Best Valid Bleu: [2.758] saved to output/ckpt_zhth/model_best_2.758!\n",
    "current checkpoints: ['model_best_0.0', 'epoch_final', 'model_best_1.239', 'model_best_2.758']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**训练参数简介**：\n",
    "\n",
    "```\n",
    "python paddleseq_cli/train.py -c xx.yaml \\\n",
    "        --amp --ngpus 1 --update-freq 4 \\\n",
    "        --max-epoch 50 --save-epoch 10 --save-dir output \\\n",
    "        --pretrained ckpt --log-steps 100 --max-tokens 4096  \\\n",
    "        --seed 1 --eval-beam\n",
    "        \n",
    "-c： 配置文件路径；\n",
    "\n",
    "--amp： 混合精度训练，可以加速训练。（使用amp后，日志中梯度范数gnorm会变得特别大，是正常现象，因为乘了scale）；\n",
    "\n",
    "--ngpus: 使用的gpu数量；\n",
    "\n",
    "--update-freq：梯度累加，可以模拟更大的batch_size,从而取得更好的分数（设为4时，日志中num_updates=step/4,bsz=bsz*4）；\n",
    "\n",
    "--max-epoch： 训练轮数；\n",
    "\n",
    "--save-epoch： 保存权重间隔的轮数；\n",
    "\n",
    "--save-dir： 模型权重、日志、可视化日志、预测输出保存路径；\n",
    "\n",
    "--pretrained： 加载预先训练的权重,是个目录，一般在output/ckpt下，目录下包含model.pdparams, model.pdopt和model.yaml；\n",
    "\n",
    "--log-steps： 打印日志的频率；\n",
    "\n",
    "--max-tokens： 每个batch最大的tokens数（source或target最大的tokens数）；\n",
    "\n",
    "--seed： 随机种子；\n",
    "\n",
    "--eval-beam： 默认False，指定后会在训练的评估时，启用beam search来生成预测结果并计算bleu分数，比默认的teacher forcing的argmax输出要更准确。\n",
    "```\n",
    "\n",
    "\n",
    "**关于恢复训练**：\n",
    "\n",
    "有时难免遇到被迫关闭训练，需要重启的情形，可以使用恢复训练，即加载以前训练的模型权重和优化器状态，直接从权重目录的model.yaml加载即可：\n",
    "\n",
    "```\n",
    "ckpt_dir=output/ckpt_zhth/epoch_final\n",
    "python paddleseq_cli/train.py -c $ckpt_dir/model.yaml \n",
    "```\n",
    "\n",
    "其他参数详见[config.py](https://github.com/MiuGod0126/PaddleSeq/blob/7932c3f34e9da25bad5d7bffb158f11e2c2b7d5d/paddleseq_cli/config.py),其实大多数参数和yaml中的重复，而命令行的参数会覆盖yaml中的参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 训练6向⭐ \n",
    "\n",
    "**一键跑通六向训练，快速上分！**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs=50\r\n",
      "freq=4 # update frequence\r\n",
      "\r\n",
      "directions=(\"zh_th\" \"th_zh\" \"zh_fr\" \"fr_zh\" \"zh_ru\" \"ru_zh\")\r\n",
      "export PYTHONWARNINGS='ignore:semaphore_tracker:UserWarning'\r\n",
      "for direct in ${directions[@]}\r\n",
      "  do\r\n",
      "      echo \"------------------------------------------------------------training ${direct}....------------------------------------------------------------\"\r\n",
      "      python paddleseq_cli/train.py -c examples/ikcest22/configs/${direct}.yaml --update-freq $freq --max-epoch $epochs\r\n",
      "  done\r\n",
      "\r\n",
      "\r\n",
      "echo \"all done\"\r\n"
     ]
    }
   ],
   "source": [
    "# 查看训练脚本，可自行修改相应参数\n",
    "!cat examples/ikcest22/scripts/train_all.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ≈10h\n",
    "!bash examples/ikcest22/scripts/train_all.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    ".----------------training zh_th.....----------------\n",
    "INFO 2022-09-04 18:24:46,576 cloud_utils.py:122] get cluster from args:job_server:None pods:['rank:0 id:None addr:127.0.0.1 port:None visible_gpu:[] trainers:[\"gpu:[\\'0\\'] endpoint:127.0.0.1:58733 rank:0\"]'] job_stage_flag:None hdfs:None\n",
    "........................\n",
    "2022-09-04 18:24:52,306 | transformer_base_rank0: Now training epoch 1. LR=0.00000\n",
    "2022-09-04 18:25:11,480 | transformer_base_rank0: Train| epoch:[1/50], step:[100/419], speed:5.22 step/s, loss:14.192, nll_loss:14.134, ppl:17973.68, bsz:944.0, gnorm:9.323, num_updates:25, lr:0.000012348\n",
    ".----------------training th_zh.....----------------\n",
    "...............................\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 四、模型评估"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 评估中泰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.args  model.pdopt  model.pdparams\r\n"
     ]
    }
   ],
   "source": [
    "# 权重目录包含配置参数、优化器参数、模型参数\n",
    "# !ls output/ckpt_zhth/epoch_final/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估验证集\n",
    "!python paddleseq_cli/generate.py -c examples/ikcest22/configs/zh_th.yaml --pretrained output/ckpt_zhth/epoch_final --test-pref datasets/bpe/zh_th/valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "2022-09-04 18:29:08 | INFO | paddleseq_cli.generate | Paddle BlEU Score:47.3688\n",
    "Sacrebleu: BLEU = 47.81 70.5/55.5/49.6/47.4 (BP = 0.868 ratio = 0.876 hyp_len = 11084 ref_len = 12647)\n",
    "write to file output/result.txt success.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 平均n个权重（需要n个model_best_xx）\n",
    "!python scripts/average_checkpoints.py --inputs output/ckpt_zhth/ --output output/ckpt_zhth/avg2 --num-ckpts  2\n",
    "!python paddleseq_cli/generate.py -c examples/ikcest22/configs/zh_th.yaml --pretrained output/ckpt_zhth/avg2  --test-pref datasets/bpe/zh_th/valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 评估6向"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directions=(\"zh_th\" \"th_zh\" \"zh_fr\" \"fr_zh\" \"zh_ru\" \"ru_zh\")\r\n",
      "data_paths=(\"zh_th\" \"zh_th\" \"zh_fr\" \"zh_fr\" \"zh_ru\" \"zh_ru\")\r\n",
      "ckpts=(\"output/ckpt_zhth/epoch_final\"\r\n",
      "        \"output/ckpt_thzh/epoch_final\"\r\n",
      "        \"output/ckpt_zhfr/epoch_final\"\r\n",
      "        \"output/ckpt_frzh/epoch_final\"\r\n",
      "        \"output/ckpt_zhru/epoch_final\"\r\n",
      "        \"output/ckpt_ruzh/epoch_final\")\r\n",
      "\r\n",
      "for ((i=0;i<${#directions[@]};i++))\r\n",
      "  do  \r\n",
      "      direct=${directions[$i]}\r\n",
      "      ckpt=${ckpts[$i]}\r\n",
      "      echo \"------------------------------------------------------------evaluate ${direct}....------------------------------------------------------------\"\r\n",
      "      python paddleseq_cli/generate.py -c examples/ikcest22/configs/${direct}.yaml --pretrained $ckpt --test-pref datasets/bpe/${data_paths[$i]}/valid\r\n",
      "  done\r\n",
      "\r\n",
      "echo \"all done\"\r\n"
     ]
    }
   ],
   "source": [
    "# 查看评估脚本，可以修改对应的权重, 如把output/ckpt_zhth/epoch_final换成output/ckpt_zhth/model_best_xxx\n",
    "!cat examples/ikcest22/scripts/evaluate_all.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3min\n",
    "!bash examples/ikcest22/scripts/evaluate_all.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    ".----------------evaluate zh_th.....----------------\n",
    "2022-09-04 18:33:02 | INFO | paddleseq_cli.generate | Paddle BlEU Score:47.3688\n",
    "Sacrebleu: BLEU = 47.81 70.5/55.5/49.6/47.4 (BP = 0.868 ratio = 0.876 hyp_len = 11084 ref_len = 12647)\n",
    "write to file output/result.txt success.\n",
    ".-----------------evaluate th_zh.....----------------\n",
    "2022-09-04 18:33:25 | INFO | paddleseq_cli.generate | Paddle BlEU Score:49.0684\n",
    "Sacrebleu: BLEU = 49.46 63.7/49.7/45.1/43.4 (BP = 0.991 ratio = 0.991 hyp_len = 10534 ref_len = 10626)\n",
    ".----------------evaluate zh_fr....----------------\n",
    "2022-09-04 18:33:55 | INFO | paddleseq_cli.generate | Paddle BlEU Score:14.9049\n",
    "Sacrebleu: BLEU = 17.84 47.2/22.6/13.5/8.8 (BP = 0.946 ratio = 0.947 hyp_len = 21648 ref_len = 22854)\n",
    "write to file output/result.txt success.\n",
    ".----------------evaluate fr_zh.....----------------\n",
    "2022-09-04 18:34:26 | INFO | paddleseq_cli.generate | Paddle BlEU Score:16.1178\n",
    "Sacrebleu: BLEU = 16.33 48.4/20.4/10.9/6.6 (BP = 1.000 ratio = 1.006 hyp_len = 19290 ref_len = 19175)\n",
    "write to file output/result.txt success.\n",
    ".----------------evaluate zh_ru.....----------------\n",
    "2022-09-04 18:35:08 | INFO | paddleseq_cli.generate | Paddle BlEU Score:10.7678\n",
    "Sacrebleu: BLEU = 14.89 39.9/17.8/10.6/6.8 (BP = 0.992 ratio = 0.992 hyp_len = 24192 ref_len = 24381)\n",
    "write to file output/result.txt success.\n",
    ".----------------evaluate ru_zh....----------------\n",
    "2022-09-04 18:35:49 | INFO | paddleseq_cli.generate | Paddle BlEU Score:16.8321\n",
    "Sacrebleu: BLEU = 16.99 48.1/21.0/11.6/7.1 (BP = 1.000 ratio = 1.000 hyp_len = 23760 ref_len = 23753)\n",
    "write to file output/result.txt success.\n",
    "all done\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 五、预测提交"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 中泰预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python paddleseq_cli/generate.py -c examples/ikcest22/configs/zh_th.yaml --pretrained output/ckpt_zhth/epoch_final --only-src\n",
    "!cat output/generate.txt | grep -P \"^H\" | sort -V | cut -f 3- > zh_th.rst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ผ้า กระเป๋า ดำ บน ผนัง ใน ทะเลทราย ฝัง กระดูก คลาสสิก และ เสน่ห์ อัน หรูหรา ปล่อย ให้ เห็น ถึง อารมณ์ ดีงาม ที่ เรียบง่าย\r\n",
      "เก็บ เอว ยก สะโพก หรูหรา\r\n",
      "เพชร มินิ ที่ ละเอียด\r\n",
      "203 LV . 7 ดาว\r\n",
      "กองทหาร ฝันร้าย · ยอดเยี่ยม\r\n",
      "ทำให้ โล่ ความ ยาว\r\n",
      "229 LV . 4 ดาว\r\n",
      "อาวุธ ม่วง ดร อป เค วส ขั้น 7 - แหวน\r\n",
      "ชื่อ : บริษัท เซี่ยงไฮ้ หัว เช่อผิ่น เปียว เท สติ้ง เทคโนโลยี จำกัด\r\n",
      "เฮ ก ซะ วา เลน ต์\r\n"
     ]
    }
   ],
   "source": [
    "!head zh_th.rst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 预测6向⭐\n",
    "\n",
    "默认使用最后一个权重epoch_final, 可以修改成model_best, 结果直接打包到trans_result.zip ，可以提交了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directions=(\"zh_th\" \"th_zh\" \"zh_fr\" \"fr_zh\" \"zh_ru\" \"ru_zh\")\r\n",
      "ckpts=(\"output/ckpt_zhth/epoch_final\"\r\n",
      "        \"output/ckpt_thzh/epoch_final\"\r\n",
      "        \"output/ckpt_zhfr/epoch_final\"\r\n",
      "        \"output/ckpt_frzh/epoch_final\"\r\n",
      "        \"output/ckpt_zhru/epoch_final\"\r\n",
      "        \"output/ckpt_ruzh/epoch_final\")\r\n",
      "\r\n",
      "for ((i=0;i<${#directions[@]};i++))\r\n",
      "  do  \r\n",
      "      direct=${directions[$i]}\r\n",
      "      ckpt=${ckpts[$i]}\r\n",
      "      echo \"------------------------------------------------------------generate ${direct}....------------------------------------------------------------\"\r\n",
      "      python paddleseq_cli/generate.py -c examples/ikcest22/configs/${direct}.yaml --pretrained $ckpt --only-src\r\n",
      "      cat output/generate.txt | grep -P \"^H\" | sort -V | cut -f 3- > ${direct}.rst\r\n",
      "  done\r\n",
      "\r\n",
      "zip -r trans_result.zip *.rst\r\n",
      "\r\n",
      "echo \"all done\"\r\n"
     ]
    }
   ],
   "source": [
    "# 可以使用验证集上评估最好的权重替代epoch_final\n",
    "!cat examples/ikcest22/scripts/generate_all.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3min\n",
    "!bash examples/ikcest22/scripts/generate_all.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    ".----------------generate zh_th....----------------\n",
    "write to file output/result.txt success.\n",
    ".----------------generate th_zh.....----------------\n",
    "write to file output/result.txt success.\n",
    ".----------------generate zh_fr.....----------------\n",
    "write to file output/result.txt success.\n",
    ".----------------generate fr_zh.....----------------\n",
    "write to file output/result.txt success.\n",
    ".----------------generate zh_ru.....----------------\n",
    "write to file output/result.txt success.\n",
    ".----------------generate ru_zh.....----------------\n",
    "2022-09-04 18:49:35 | INFO | paddleseq_cli.generate | configs:\n",
    "write to file output/result.txt success.\n",
    "  adding: fr_zh.rst (deflated 59%)\n",
    "  adding: ru_zh.rst (deflated 61%)\n",
    "  adding: th_zh.rst (deflated 56%)\n",
    "  adding: zh_fr.rst (deflated 64%)\n",
    "  adding: zh_ru.rst (deflated 74%)\n",
    "  adding: zh_th.rst (deflated 98%)\n",
    "all done\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 提交结果\n",
    "\n",
    "提交格式\n",
    "\n",
    "**1.翻译结果文件命名**\n",
    "\n",
    "各翻译方向结果文件需按指定方式命名。\n",
    "\n",
    "法文-中文方向：fr_zh.rst\n",
    "\n",
    "俄文-中文方向：ru_zh.rst\n",
    "\n",
    "泰文-中文方向：th_zh.rst\n",
    "\n",
    "中文-法文方向：zh_fr.rst\n",
    "\n",
    "中文-俄文方向：zh_ru.rst\n",
    "\n",
    "中文-泰文方向：zh_th.rst\n",
    "\n",
    "**2.翻译结果文件格式**\n",
    "\n",
    "翻译结果文件按行存储目标端句子。\n",
    "\n",
    "**3.打包提交**\n",
    "\n",
    "将所有翻译结果文件压缩为单个zip文件，参考如下操作：\n",
    "\n",
    "```\n",
    "zip trans_result.zip *.rst\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./trans_result.zip\r\n"
     ]
    }
   ],
   "source": [
    "!ls ./trans_result*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**generate_all.sh脚本自动打包了所有结果，直接将trans_result.zip提交就行了。**\n",
    "\n",
    "[提交地址](https://aistudio.baidu.com/aistudio/competition/detail/477/0/submit-result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本项目baseline：\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/e288d633f45746fe81929def49554a05bc90c330a2af457ebf936d0d366baaa1)\n",
    "\n",
    "官方baseline：\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/a1b95bae8a7f4e728b561488d24a82ccd7c7e87628d545ba94915e1fe47b266f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 六、总结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 本项目完成了数据处理、加载，模型训练、评估、预测以及一键生成提交文件，方便大家快速上榜。 对于进一步的优化，可以从以下几点入手：\n",
    "\n",
    "#### 1. 数据： ①对于英俄文本未进行任何处理，可以尝试用moses进行tokenize和truecase   ②采用回译，自训练等方法合成更多数据   ③或者用语言模型挖掘与IKCEST数据分布更接近的数据，与原数据混合训练； ④加数据！加卡！\n",
    "#### 2. 模型： 尝试其他的结构，如transformerbig，或自己魔改参数；\n",
    "#### 3. 训练： 多语言训练、或自己预训练；\n",
    "#### 4. 损失：使用simcut、rdrop等增强方法；\n",
    "#### 5. 预测：① 如简单修改beam大小； ②或者使用噪声通道重排，利用一个反向翻译模型和一个语言模型来对前向模型预测的多个结果进行重排。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 如果项目中有任何问题，欢迎加入比赛群讨论，共同进步！ \n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/f7be20c125b64422aa95fa340720291d454497b935d64eceb98fe05956361cc3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 最后，哈哈，各位冠军选手给俺点给star吧！\n",
    "\n",
    "#### [PaddleSeq](https://github.com/MiuGod0126/PaddleSeq/blob/main/examples/ikcest22/README.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 代码参考：\n",
    "\n",
    "[1.PaddleNLP](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/examples/machine_translation)\n",
    "\n",
    "[2.fairseq](https://github.com/pytorch/fairseq)\n",
    "\n",
    "[3.ConvS2S_Paddle](https://github.com/MiuGod0126/ConvS2S_Paddle)\n",
    "\n",
    "[4.STACL_Paddle](https://github.com/MiuGod0126/STACL_Paddle)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
